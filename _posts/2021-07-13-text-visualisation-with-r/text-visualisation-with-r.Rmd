---
title: "Text visualisation with R"
description: |
  Yesterday, we learnt how to visualise text (unstructured) data.
author:
  - name: Nurulasyiqah Md. Taha
    url: https://www.linkedin.com/in/nurulasyiqah-md-taha/
date: 07-12-2021
output:
  distill::distill_article:
    self_contained: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.retina=3,
                      echo = TRUE, 
                      eval = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

### Installing and launching R packages

```{r}
packages = c('tidytext', 'tidyverse', 'dplyr', 'ggraph',
             'widyr', 'wordcloud', 'ggwordcloud', 'DT',
             'textplot', 'lubridate', 'hms', 'tidygraph',
             'igraph')
              
for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}
```

### Data preparation

We will work with multiple folders containing multiple text files (documents) and learn how to extract and manipulate text data. 

One raw data have been uploaded, raw data should be deleted.

### Import multiple text files from multiple folders

**Step 1**: Create a folder list

```{r eval=FALSE}
news20 <- "data/20news/"
```

**Step 2**: Define a function to read all files from a folder into a data frame

```{r eval=FALSE}
read_folder <- function(infolder) { #writing a function
  tibble(file = dir(infolder,
                    full.names = TRUE)) %>%
    mutate(text = map(file,
                      read_lines)) %>%
    transmute(id = basename(file),
              text) %>%
    unnest(text)
}
```

Note that we don't use if-loop function to look through the files, but "infolder" function. It is similar to writing a R code - nothing will happen till we run Step 3.

**Step 3**: Reading all messages from the 20news folder

```{r eval=FALSE}
raw_text <- tibble(folder = dir(news20, full.names = TRUE)) %>%
  mutate(folder_out = map(folder, read_folder)) %>%
  unnest(cols = c(folder_out)) %>%
  transmute(newsgroup = basename(folder), id, text)
write_rds(raw_text,"data/rds/news20.rds") #output a consolidated data set of raw text in .rds format
```

Note that no. of observations is 7,601.

```{r}
raw_text <- read_rds("data/rds/news20.rds")

glimpse(raw_text)
```

We should now delete all the raw data folders.

### Initial EDA

To check that all raw text have been written into rds:

```{r}
raw_text %>%
  group_by(newsgroup) %>%
  summarise(messages = n_distinct(id)) %>% #count the unique article ID
  ggplot(aes(messages, newsgroup)) +
#x-axis is no of articles, y-axis is the different news outlets
  geom_col(fill = "lightblue") +
  labs(y = NULL)
```


### Cleaning text data

**Step 1**: To remove header and automatic email signatures

```{r}
cleaned_text <- raw_text %>%
  group_by(newsgroup, id) %>%
  filter(cumsum(text == "") > 0,
         cumsum(str_detect(text, "^--")) == 0) %>%
  #to detect string based on your defined pattern
  ungroup()
```

In RStudio "Environment" tab, click on cleaned_text to skim through what needs to be removed next.

**Step 2**: Remove lines with nested text representing quotes from other users.

```{r}
cleaned_text <- cleaned_text %>%
  filter(str_detect(text, "^[^>]+[A-Za-z\\d]")
         | text == "",
         !str_detect(text, "writes(:|\\.\\.\\.)$"),
         !str_detect(text, "^In article <")
  )
```

Note that no. of observations reduced to 4,449.

```{r}
glimpse(cleaned_text)
```

You need to clean the text further because if you glimpse at cleaned_text, text is still dirty.

Refer to [stringr regular expressions](https://stringr.tidyverse.org/articles/regular-expressions.html) for ideas on patterns to define strings to remove.

### Text data processing

Use [unnest_tokens()](https://www.rdocumentation.org/packages/tidytext/versions/0.3.1/topics/unnest_tokens) to split dataset into tokens and [stop_words()](https://rdrr.io/cran/tidytext/man/stop_words.html) to remove stop-words.

```{r}
usenet_words <- cleaned_text %>%
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z']$"),
         !word %in% stop_words$word)
```

Note that no. of observations increased to 11,825 because now we are looking at words instead of article lines.

```{r}
glimpse(usenet_words)
```

You still need to look through the text data and clean further words.

### Exploring common words

Before we create a word cloud, we can calculate the word distribution from the entire tokenised dataset.

```{r}
usenet_words %>%
  count(word, sort = TRUE)
```

Alternatively, we can count words within each newsgroup.

```{r}
words_by_newsgroup <- usenet_words %>%
  count(newsgroup, word, sort = TRUE) %>% #count generates n column and sort in descending order
  ungroup()

glimpse(words_by_newsgroup)
```

### Visualising common words using wordcloud()

Use the following code chunk:

```{r}
wordcloud(words_by_newsgroup$word,
          words_by_newsgroup$n,
          max.words = 150)
```

A [dynamic data table](https://vasq.netlify.app/posts/2021-07-04-first-data-visualisation-with-r/#how-to-create-an-interactive-data-table) can be used to complement the wordcloud above.

### Visualising common words using ggwordcloud()

We can also plot wordcloud using [ggwordcloud](https://lepennec.github.io/ggwordcloud/articles/ggwordcloud.html) package under ggplot so that we can facet by newsgroup.

```{r}
set.seed(1234)

words_by_newsgroup %>%
  filter(n > 0) %>%
ggplot(aes(label = word, size = n)) +
  geom_text_wordcloud() +
  theme_minimal() +
  facet_wrap(~newsgroup)
```

### Visualising TF-IDF

**Step 1**: Compute tf-idf within newsgroups

If a word appears in more documents, it doesn't necessarily mean that it is more important.

Use the following code chunk to perform inverse document frequency and group by newsgroup:

```{r}
tf_idf <- words_by_newsgroup %>%
  bind_tf_idf(word, newsgroup, n) %>%
  arrange(desc(tf-idf))
```

**Step 2**: Visualising tf-idf as an interactive table

```{r}
DT::datatable(tf_idf, filter = 'top') %>% #to insert filter box at the top
  formatRound(columns = c('tf', 'idf', 'tf_idf'),
              digits = 3) %>% #round up to 3 dp
  formatStyle(0, target = 'row', lineHeight='25%') #reduce until 25%
```

**Step 3**: Use facet barcharts technique to visualise the tf-idf values of sci-related newsgroups.

```{r}
tf_idf %>%
  filter(str_detect(newsgroup, "^sci\\.")) %>% #to look for all newsgroups with 'sci'
  group_by(newsgroup) %>%
  slice_max(tf_idf, n = 12) %>%
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(tf_idf, word, fill = newsgroup)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ newsgroup, scales = "free") + #x-axis scale of each graph not standardised
  labs(x = "tf-idf", y = NULL) #all 
```

### Visualising correlation as a network

We would like to find newsgroups similar to each other based on words they use in their articles.

**Step 1**: Transpose the words_by_newsgroup table to form a matrix and use pairwise correlation

```{r}
newsgroup_cors <- words_by_newsgroup %>%
  pairwise_cor(newsgroup, word, n, sort = TRUE)

glimpse(newsgroup_cors)
```

We can perform pairwise correlation on the tf-idf table too, depending on what outcome we desire.

**Step 2**: Display correlation in graph form

```{r}
set.seed(2017)

newsgroup_cors %>%
  filter(correlation > 0.025) %>% #only focus on pairs with correlation >0.025
  graph_from_data_frame() %>% #in a graph data model
  ggraph(layout = "fr") +
  geom_edge_link(aes(alpha = correlation, #intensity
                     width = correlation)) + #edge weight
  geom_node_point(size = 6, color = "lightblue") +
  geom_node_text(aes(label = name), color = "red", repel = TRUE) +
  theme_void()
```

Note that graph cannot show negative correlation, so we should set a threshold for correlation >0 to serve as the weight.

The graph does not show which words caused two newsgroups to be considered to be similar. We should complement this with a DT datatable to find out what those words are.

### Looking at phrases (n-grams)

**Step 1**: Use unnest_tokens()

```{r}
bigrams <- cleaned_text %>%
  unnest_tokens(bigram, #output called 'bigram'
                text, #search through text column
                token = "ngrams", 
                n = 2) #bigram is n=2

glimpse(bigrams)
```

**Step 2**: Separate the bigram into two words

```{r}
bigrams_separated <- bigrams %>%
  filter(bigram != 'NA') %>% #excludes bigrams w NA
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

glimpse(bigrams_filtered)
```

**Step 3**: Create network graph from bigram data frame

```{r}
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)
```

```{r}
bigram_graph <- bigram_counts %>%
  filter(n>3) %>%
  graph_from_data_frame()
bigram_graph
```

```{r}
set.seed(1234)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name),
                 vjust = 1,
                 hjust = 1)
```

Note because the data set is not thoroughly cleaned, we have a v-v pair, which is not meaningful.

To make it more meaningful, we can use arrows:

```{r}
set.seed(1234)

#use grid package to build arrow
a <- grid::arrow(type = "closed",
                 length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n),
                 show.legend = FALSE,
                 arrow = a,
                 end_cap = circle(.05, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name),
                 vjust = 1,
                 hjust = 1) +
  theme_void()
```

---

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.
